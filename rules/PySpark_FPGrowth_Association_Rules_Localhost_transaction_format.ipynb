{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.15.4:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FpGrowth</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=FpGrowth>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "conf = pyspark.SparkConf().setAppName(\"FpGrowth\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n",
      "|id_transaction|product_name|\n",
      "+--------------+------------+\n",
      "|             0|     T-shirt|\n",
      "|             0|    Trousers|\n",
      "|             0|        Belt|\n",
      "|             1|     T-shirt|\n",
      "|             1|      Jacket|\n",
      "|             2|      Jacket|\n",
      "|             2|      Gloves|\n",
      "|             3|     T-shirt|\n",
      "|             3|    Trousers|\n",
      "|             3|        Belt|\n",
      "|             4|     T-shirt|\n",
      "|             4|    Trousers|\n",
      "|             4|    Sneakers|\n",
      "|             4|      Jacket|\n",
      "|             4|        Belt|\n",
      "|             5|      Jacket|\n",
      "|             5|        Belt|\n",
      "|             6|    Trousers|\n",
      "|             6|    Sneakers|\n",
      "|             6|        Belt|\n",
      "+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true',\n",
    "                                                                inferschema='true').load('transactional_tshirt.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------------------------------+\n",
      "|id_transaction|items                                      |\n",
      "+--------------+-------------------------------------------+\n",
      "|1             |[T-shirt, Jacket]                          |\n",
      "|6             |[Sneakers, Belt, Trousers]                 |\n",
      "|3             |[Belt, T-shirt, Trousers]                  |\n",
      "|5             |[Belt, Jacket]                             |\n",
      "|4             |[Sneakers, Belt, T-shirt, Trousers, Jacket]|\n",
      "|8             |[Belt, T-shirt, Trousers, Jacket]          |\n",
      "|7             |[Sneakers, Belt, Trousers]                 |\n",
      "|2             |[Gloves, Jacket]                           |\n",
      "|0             |[Belt, T-shirt, Trousers]                  |\n",
      "+--------------+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Organize the data by shopping basket\n",
    "from pyspark.sql.functions import collect_set, col, count\n",
    "baskets = df.groupBy('id_transaction').agg(collect_set('product_name').alias('items'))\n",
    "baskets.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.4, minConfidence=0.6)\n",
    "model = fpGrowth.fit(baskets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|               items|freq|\n",
      "+--------------------+----+\n",
      "|              [Belt]|   7|\n",
      "|          [Trousers]|   6|\n",
      "|    [Trousers, Belt]|   6|\n",
      "|           [T-shirt]|   5|\n",
      "| [T-shirt, Trousers]|   4|\n",
      "|[T-shirt, Trouser...|   4|\n",
      "|     [T-shirt, Belt]|   4|\n",
      "|            [Jacket]|   5|\n",
      "+--------------------+----+\n",
      "\n",
      "+-------------------+----------+------------------+------------------+\n",
      "|         antecedent|consequent|        confidence|              lift|\n",
      "+-------------------+----------+------------------+------------------+\n",
      "|    [T-shirt, Belt]|[Trousers]|               1.0|               1.5|\n",
      "|[T-shirt, Trousers]|    [Belt]|               1.0|1.2857142857142856|\n",
      "|         [Trousers]|    [Belt]|               1.0|1.2857142857142856|\n",
      "|         [Trousers]| [T-shirt]|0.6666666666666666|               1.2|\n",
      "|   [Trousers, Belt]| [T-shirt]|0.6666666666666666|               1.2|\n",
      "|             [Belt]|[Trousers]|0.8571428571428571|1.2857142857142858|\n",
      "|          [T-shirt]|[Trousers]|               0.8|1.2000000000000002|\n",
      "|          [T-shirt]|    [Belt]|               0.8|1.0285714285714287|\n",
      "+-------------------+----------+------------------+------------------+\n",
      "\n",
      "+--------------+--------------------+----------------+\n",
      "|id_transaction|               items|      prediction|\n",
      "+--------------+--------------------+----------------+\n",
      "|             1|   [T-shirt, Jacket]|[Trousers, Belt]|\n",
      "|             6|[Sneakers, Belt, ...|       [T-shirt]|\n",
      "|             3|[Belt, T-shirt, T...|              []|\n",
      "|             5|      [Belt, Jacket]|      [Trousers]|\n",
      "|             4|[Sneakers, Belt, ...|              []|\n",
      "|             8|[Belt, T-shirt, T...|              []|\n",
      "|             7|[Sneakers, Belt, ...|       [T-shirt]|\n",
      "|             2|    [Gloves, Jacket]|              []|\n",
      "|             0|[Belt, T-shirt, T...|              []|\n",
      "+--------------+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exibe os itensfrequentes\n",
    "model.freqItemsets.show()\n",
    "\n",
    "# Exibe as association rules geradas.\n",
    "model.associationRules.show()\n",
    "\n",
    "# transforma e examina os antecedentes, consequentes, confidence e lift.\n",
    "model.transform(baskets).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and other contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Official Documentation Spark - Frequent Pattern Mining - Fp-Growth <br>\n",
    "https://spark.apache.org/docs/latest/ml-frequent-pattern-mining.html#fp-growth<br>\n",
    "How to set up PySpark for your Jupyter notebook<br>\n",
    "https://opensource.com/article/18/11/pyspark-jupyter-notebook<br>\n",
    "MLlib is Apache Spark's scalable machine learning library.<br>\n",
    "https://spark.apache.org/mllib/<br>\n",
    "Frequent Itemsets via the FP-Growth Algorithm (comp: Apriori mlxtend)<br>\n",
    "http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/fpgrowth/#example-1-generating-frequent-itemsets<br>\n",
    "An example demonstrating FPGrowth<br>\n",
    "https://github.com/apache/spark/blob/master/examples/src/main/python/ml/fpgrowth_example.py<br>\n",
    "Extracting, transforming and selecting features<br>\n",
    "https://spark.apache.org/docs/latest/ml-features<br>\n",
    "FPGrowth.train<br>\n",
    "https://spark.apache.org/docs/2.1.0/mllib-frequent-pattern-mining.html<br>\n",
    "Simplify Market Basket Analysis using FP-growth on Databricks:<br>\n",
    "https://databricks.com/blog/2018/09/18/simplify-market-basket-analysis-using-fp-growth-on-databricks.html<br>\n",
    "Market Basket Analysis on 3 million orders from Instacart using Spark.<br>\n",
    "https://medium.com/analytics-vidhya/market-basket-analysis-on-3-million-orders-from-instacart-using-spark-24cc6469a92e\n",
    "<br>\n",
    "R-Apriori: An Efficient Apriori based Algorithm on Spark:<br>\n",
    "https://iith.ac.in/~mkaul/papers/pikm09-rathee.pdf<br>\n",
    "Optimize conversion between Apache Spark and pandas DataFrames<br>\n",
    "https://docs.databricks.com/spark/latest/spark-sql/spark-pandas.html<br>\n",
    "FP-growth algorithm (implementation sample and comments)<br>\n",
    "http://qtdynamics.com/index.php/content/index/pid/276/cid/6533.html<br>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
